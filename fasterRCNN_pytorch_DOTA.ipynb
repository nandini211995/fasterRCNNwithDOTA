{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fasterRCNN_pytorch_DOTA.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHDAE-DzbV_7"
      },
      "source": [
        "## Data Format :\n",
        "train \n",
        "\n",
        " |--images\n",
        "\n",
        " |--labelTxt\n",
        "\n",
        "\n",
        "During training, the model expects both the input tensors, as well as a targets (list of dictionary), containing:\n",
        "\n",
        "- boxes (FloatTensor[N, 4]): the ground-truth boxes in [x1, y1, x2, y2] format, with 0 <= x1 < x2 <= W and 0 <= y1 < y2 <= H.\n",
        "     \n",
        "     **I think here [x1, y1, x2, y2] is [x_min, y_min, x_max, y_max]**\n",
        "\n",
        "- labels (Int64Tensor[N]): the class label for each ground-truth box\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5OayC2Tcrev"
      },
      "source": [
        "#STEP1 : Read images as in tensor form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ca9nq6bVvWqR"
      },
      "source": [
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from PIL import Image\n",
        "from xml.dom.minidom import parse\n",
        "%matplotlib inline"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NF_IaP3Ndp7s"
      },
      "source": [
        "class MarkDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms=None):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"images\"))))\n",
        "        self.bbox_xml = list(sorted(os.listdir(os.path.join(root, \"Annotations_new\"))))\n",
        " \n",
        "    def __getitem__(self, idx):\n",
        "        # load images and bbox\n",
        "        img_path = os.path.join(self.root, \"images\", self.imgs[idx])\n",
        "        bbox_xml_path = os.path.join(self.root, \"Annotations_new\", self.bbox_xml[idx])\n",
        "        img = Image.open(img_path).convert(\"RGB\")        \n",
        "        \n",
        "        # Read file, VOC format dataset label is xml format file\n",
        "        dom = parse(bbox_xml_path)\n",
        "        # Get Document Element Object\n",
        "        data = dom.documentElement\n",
        "        # Get objects\n",
        "        objects = data.getElementsByTagName('object')        \n",
        "        # get bounding box coordinates\n",
        "        boxes = []\n",
        "        labels = []\n",
        "        label_codes = {'plane': 15, 'baseball-diamond' :1, 'bridge':2, 'ground-track-field':3, 'small-vehicle':4, 'large-vehicle':5, 'ship':6, 'tennis-court':7,\n",
        "                      'basketball-court':8, 'storage-tank':9,  'soccer-ball-field':10, 'roundabout':11, 'harbor':12, 'swimming-pool':13, 'helicopter':14}\n",
        "        for object_ in objects:\n",
        "            # Get the contents of the label\n",
        "            name = object_.getElementsByTagName('name')[0].childNodes[0].nodeValue  # Is label, mark_type_1 or mark_type_2\n",
        "            labels.append(label_codes[name] ) # Background label is 0, mark_type_1 and mark_type_2 labels are 1 and 2, respectively\n",
        "            \n",
        "            bndbox = object_.getElementsByTagName('bndbox')[0]\n",
        "            xmin = np.float(bndbox.getElementsByTagName('x1')[0].childNodes[0].nodeValue)\n",
        "            ymin = np.float(bndbox.getElementsByTagName('y1')[0].childNodes[0].nodeValue)\n",
        "            xmax = np.float(bndbox.getElementsByTagName('x2')[0].childNodes[0].nodeValue)\n",
        "            ymax = np.float(bndbox.getElementsByTagName('y2')[0].childNodes[0].nodeValue)\n",
        "            boxes.append([xmin, ymin, xmax, ymax])     \n",
        " \n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
        "        \n",
        "        labels = torch.as_tensor(labels, dtype=torch.int64)        \n",
        " \n",
        "        image_id = torch.tensor([idx])\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((len(objects),), dtype=torch.int64)\n",
        " \n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = labels\n",
        "        # Since you are training a target detection network, there is no target [masks] = masks in the tutorial\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        " \n",
        "        #if self.transforms is not None:\n",
        "            # Note that target (including bbox) is also transformed\\enhanced here, which is different from transforms from torchvision import\n",
        "            # Https://github.com/pytorch/vision/tree/master/references/detectionOfTransforms.pyThere are examples of target transformations when RandomHorizontalFlip\n",
        "            #img, target = self.transforms(img, target)\n",
        " \n",
        "        return img, target\n",
        " \n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AfQ5caXtiBxk"
      },
      "source": [
        "root = r'/content/drive/MyDrive/Dotadatasetsample/train/'"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9z5LcVfiPhA"
      },
      "source": [
        "num_classes = 16"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pk916XF-iXh0"
      },
      "source": [
        "dataset = MarkDataset(root)"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtJIVzIsifWN",
        "outputId": "56390396-8799-4427-f704-57234c36c2e3"
      },
      "source": [
        "len(dataset)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlnOvTKqv3Vn",
        "outputId": "a6611c05-c61a-4b81-f5f1-edbb41c39ecd"
      },
      "source": [
        "dataset[0]"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<PIL.Image.Image image mode=RGB size=693x890 at 0x7F7D095478D0>,\n",
              " {'area': tensor([  1015.,    864.,    992.,   1254.,   1056.,   1224.,   1225.,   1296.,\n",
              "             812., 153652.]),\n",
              "  'boxes': tensor([[410., 722., 445., 751.],\n",
              "          [434., 850., 466., 877.],\n",
              "          [284., 557., 315., 589.],\n",
              "          [387., 445., 420., 483.],\n",
              "          [234., 402., 266., 435.],\n",
              "          [433., 343., 467., 379.],\n",
              "          [516., 286., 551., 321.],\n",
              "          [498., 261., 534., 297.],\n",
              "          [447., 222., 475., 251.],\n",
              "          [126., 227., 485., 655.]]),\n",
              "  'image_id': tensor([0]),\n",
              "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
              "  'labels': tensor([ 4,  4,  4,  4,  4,  4,  4,  4,  4, 11])})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8a8gyrjkkMrS"
      },
      "source": [
        "dataset_test = MarkDataset(root )"
      ],
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZL8Tdh9DkuLB"
      },
      "source": [
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=2, shuffle=True, # num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lnx-7enhk1Ui"
      },
      "source": [
        "data_loader_test = torch.utils.data.DataLoader(\n",
        "    dataset_test, batch_size=2, shuffle=False, # num_workers=4,\n",
        "    collate_fn=utils.collate_fn)"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaPvfO7ju9NO"
      },
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvQv5hs12bOu"
      },
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        " \n",
        "      \n",
        "def get_object_detection_model(num_classes):\n",
        "    # load an object detection model pre-trained on COCO\n",
        "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
        "    \n",
        "    # replace the classifier with a new one, that has num_classes which is user-defined\n",
        "    num_classes = 16  # 16 class (plane ,......,helicopter) + background\n",
        " \n",
        "    # get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        " \n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        " \n",
        "    return model"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIFpOy63lCFa"
      },
      "source": [
        "# get the model using our helper function\n",
        "model = get_object_detection_model(num_classes = 16)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6oxffq3_1xvM"
      },
      "source": [
        "# move model to the right device\n",
        "model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHpMMGv5lUUs"
      },
      "source": [
        "# construct an optimizer\n",
        "params = [p for p in model.parameters() if p.requires_grad]\n",
        "\n",
        "# SGD\n",
        "optimizer = torch.optim.SGD(params, lr=0.0003,\n",
        "                            momentum=0.9, weight_decay=0.0005)\n"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNy7LEMMlb_6"
      },
      "source": [
        "# and a learning rate scheduler\n",
        "# cos learning rate\n",
        "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=1, T_mult=2)\n",
        "\n",
        "# let's train it for   epochs\n",
        "num_epochs = 2"
      ],
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fq21xFCLnWvz",
        "outputId": "14993654-271c-4e85-ffd9-8da3fdbb72de"
      },
      "source": [
        "cd '/content/drive/MyDrive/fasterRCNN/'"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/fasterRCNN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OJiHVP2_lq4m"
      },
      "source": [
        "from engine import train_one_epoch, evaluate"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        },
        "id": "8ASVZb8IlfbZ",
        "outputId": "027990c8-f7af-4c81-f94d-cdf0e89d7cbc"
      },
      "source": [
        "for epoch in range(num_epochs):\n",
        "    # train for one epoch, printing every 10 iterations\n",
        "    # Engine.pyTrain_ofOne_The epoch function takes both images and targets. to(device)\n",
        "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=50)\n",
        "\n",
        "    # update the learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    # evaluate on the test dataset    \n",
        "    evaluate(model, data_loader_test, device=device)    \n",
        "    \n",
        "    print('')\n",
        "    print('==================================================')\n",
        "    print('')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-114-818b94ebb81b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# train for one epoch, printing every 10 iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Engine.pyTrain_ofOne_The epoch function takes both images and targets. to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# update the learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/fasterRCNN/engine.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/fasterRCNN/engine.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetric_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_every\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'Image' object has no attribute 'to'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-HVzluwmVpa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUFB-TVy5H_l"
      },
      "source": [
        "#adversarial attack"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP9yZeH_5NFM"
      },
      "source": [
        "!pip install adversarial-robustness-toolbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdhqOhtA5T2d"
      },
      "source": [
        "from art.attacks.evasion import FastGradientMethod\n",
        "from art.estimators.object_detection import PyTorchFasterRCNN \n",
        "from art.estimators.classification import PyTorchClassifier"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RObVdbb35bpm"
      },
      "source": [
        "detector = PyTorchFasterRCNN(model = model)"
      ],
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLzKmpJD5fo8"
      },
      "source": [
        "attack = FastGradientMethod(estimator=detector, eps=0.2)"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "C0qsH7ZA5it3",
        "outputId": "640b4ceb-f25f-4c5e-8eff-79202bb6a177"
      },
      "source": [
        "x_test_adv = attack.generate()"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-128-6ab8ab1ec032>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_test_adv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'boxes'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2l_Q-cV_5peo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}